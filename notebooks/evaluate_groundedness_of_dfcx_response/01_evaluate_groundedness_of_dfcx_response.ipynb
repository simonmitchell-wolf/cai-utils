{"cells":[{"cell_type":"markdown","metadata":{"id":"J187m5erZF1R"},"source":["# Groundedness Evalutation for LLM Responses\n","\n","This notebook is set up as a basic example of evaluating how grounded LLM responses (specifically from Dialogflow CX) are based on a given test set of question/answer pairs. The question is used as the test input and the generated output is compared to the answer as ground truth, generating a boolean value and an explanation of the evaluation."]},{"cell_type":"markdown","metadata":{"id":"iI9JQN765CYH"},"source":["# SCRAPI Installation & GCP Setup\n","\n","Installs the SCRAPI library for scripting Dialogflow CX and sets up a connection to the needed projects and agents."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sXR_xGpInlmh"},"outputs":[],"source":["import importlib\n","\n","# list of packages to install\n","packages = [\n","    \"dfcx-scrapi\",\n","    \"google-auth\",\n","    \"google-cloud-aiplatform\",\n","    \"pandas\",\n","    \"tqdm\",\n","]\n","\n","# Install dependences\n","install = False\n","for package in packages:\n","    if not importlib.util.find_spec(package[0]):\n","        print(f\"Installing {package[1]}...\")\n","        install = True\n","        %pip install {package[1]} -U -q\n","\n","# Restart kernel if needed\n","if install:\n","    import IPython\n","    app = IPython.Application.instance()\n","    app.kernel.do_shutdown(True)\n","\n","print(\"Done installing.\")"]},{"cell_type":"markdown","metadata":{},"source":["# Authentication\n","\n","There are a few different methods for authenticating this notebook.\n"]},{"cell_type":"markdown","metadata":{},"source":["### Local Auth\n","\n","To run this locally, make sure you have the Google Cloud SDK installed and Application Default Credentials configured. No additional authorization should be necessary."]},{"cell_type":"markdown","metadata":{},"source":["### Auth in Google Colab\n","\n","To authenticate as the current Google user in a Colab notebook, run this cell:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import sys\n","\n","if \"google.colab\" in sys.modules:\n","    from google.colab.auth import authenticate_user\n","    authenticate_user()"]},{"cell_type":"markdown","metadata":{},"source":["### Alternate User Login\n","\n","Use the below instead to manually authenticate with a different Google user (service account impersonation TK).\n","- Run it and follow the instructions to open the link, sign in with your *target* GCP account.\n","- After clicking through to allow access, it will give you a code which you should copy back to the field outputted by the cell below.\n","- You may see a warning but it's safe to ignore and won't mess anything up."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# import os\n","# os.environ[\"PROJECT_ID\"] = SOURCE_PROJECT_NAME\n","\n","# # Interactive prompt to auth in browser\n","# !gcloud auth application-default login --no-launch-browser\n","\n","# # Set active project to the `project_id` above\n","# !gcloud auth application-default set-quota-project $PROJECT_ID"]},{"cell_type":"markdown","metadata":{"id":"r-JUOrcV5Zid"},"source":["# Define Evaluator Class\n","\n","No code actually run here, just setting up the class. Usage example below."]},{"cell_type":"code","execution_count":232,"metadata":{"executionInfo":{"elapsed":159,"status":"ok","timestamp":1718655897096,"user":{"displayName":"Simon Mitchell-Wolf","userId":"06079501617467572736"},"user_tz":240},"id":"3qx9qWHvv13s"},"outputs":[],"source":["import time\n","import vertexai\n","from vertexai.generative_models import GenerativeModel\n","import pandas as pd\n","from typing import Any, Optional\n","from tqdm import tqdm\n","from dfcx_scrapi.core.sessions import Sessions\n","import proto\n","import json\n","from google.auth import default\n","import re\n","\n","class BotEvaluator:\n","    \"\"\"\n","    A class for evaluating a chatbot's performance.\n","\n","    Args:\n","        project_id (str): The ID of the project.\n","        vertexai_region (str): The region of the Vertex AI instance.\n","        agent_id (str): The full ID of the agent.\n","            Format: `projects/<PROJECT_ID>/locations/<REGION>/agents/<AGENT_ID>`.\n","        sessions_client (Optional[Sessions]): The sessions client. Defaults to None.\n","\n","    Attributes:\n","        agent_id (str): The ID of the agent.\n","        sessions_client (Sessions): The DFCX sessions client.\n","\n","    Methods:\n","        evaluate: Evaluates the chatbot's performance on a dataset.\n","        _run_test: Runs a test on the chatbot using a dataset.\n","        _get_response: Retrieves the chatbot's response for a given query.\n","        format_response_messages: Formats the response messages received from the chatbot.\n","        check_response_groundedness: Checks the groundedness of the chatbot's response.\n","\n","    \"\"\"\n","    def __init__(\n","        self,\n","        project_id: str,\n","        vertexai_region: str,\n","        agent_id: str,\n","        sessions_client: Optional[Sessions] = None,\n","    ) -> None:\n","\n","        self.agent_id = agent_id\n","        self.sessions_client = sessions_client or Sessions()\n","\n","        vertexai.init(\n","            project=project_id,\n","            location=vertexai_region,\n","            credentials=default()[0],\n","        )\n","\n","    def evaluate(\n","        self,\n","        dataset: Optional[pd.DataFrame] = None,\n","        input_columns: Optional[list[str]] = None,\n","        check_groundedness: bool = False,\n","        ground_truth_column: Optional[str] = None,\n","        groundedness_model: Optional[str] = None,\n","    ) -> pd.DataFrame:\n","        \"\"\"\n","        Evaluates the chatbot's performance on a dataset.\n","\n","        Args:\n","            dataset (Optional[pd.DataFrame]): The dataset to evaluate. Defaults to None.\n","            input_columns (Optional[list[str]]): The input columns to use. Defaults to None.\n","            check_groundedness (bool): Whether to check the groundedness of the responses. Defaults to False.\n","            ground_truth_column (Optional[str]): The column containing the ground truth values. Defaults to None.\n","            groundedness_model (Optional[str]): The LM to use for checking groundedness. Defaults to None.\n","\n","        Returns:\n","            pd.DataFrame: The evaluated results.\n","\n","        \"\"\"\n","\n","        if dataset is None:\n","            with open(\"test_set.csv\", \"r\") as f:\n","                dataset = pd.read_csv(f)\n","\n","        if input_columns is None:\n","            input_columns = [\"question\"]\n","\n","        results = dataset.copy()\n","\n","        test_results = self._run_test(\n","            dataset=dataset,\n","            input_columns=input_columns,\n","            check_groundedness=check_groundedness,\n","            ground_truth_column=ground_truth_column,\n","            groundedness_model=groundedness_model,\n","        )\n","\n","        results = pd.merge(dataset, test_results, left_index=True, right_index=True)\n","\n","        return results\n","\n","\n","    def _run_test(\n","        self,\n","        dataset: pd.DataFrame,\n","        input_columns: str | list[str],\n","        check_groundedness: bool = False,\n","        ground_truth_column: Optional[str] = None,\n","        groundedness_model: Optional[str] = None,\n","    ) -> pd.DataFrame:\n","        \"\"\"\n","        Runs a test on the chatbot using a dataset.\n","\n","        Args:\n","            dataset (pd.DataFrame): The dataset to use for testing.\n","            input_columns (str | list[str]): The input columns to use.\n","            check_groundedness (bool): Whether to check the groundedness of the responses. Defaults to False.\n","            ground_truth_column (Optional[str]): The column containing the ground truth values. Defaults to None.\n","            groundedness_model (Optional[str]): The groundedness model to use. Defaults to None.\n","\n","        Returns:\n","            pd.DataFrame: The test results.\n","\n","        \"\"\"\n","\n","        if check_groundedness and ground_truth_column is None:\n","            raise ValueError(\n","                \"Provide both a `ground_truth_column` to check groundedness.\"\n","            )\n","\n","\n","        if isinstance(input_columns, str):\n","            input_columns = [input_columns]\n","\n","        fields = [\n","            \"response\",\n","            \"time\",\n","        ]\n","\n","        if check_groundedness:\n","            fields.extend([\"groundedness\", \"groundedness_reasoning\"])\n","\n","        columns = [\n","            f\"{input}_{field}\"\n","            for input in input_columns\n","            for field in fields\n","        ]\n","\n","        results = pd.DataFrame(\n","            index=dataset.index,\n","            columns=columns\n","        )\n","\n","        for id, example in tqdm(dataset.iterrows()):\n","\n","            for input in input_columns:\n","\n","                response, response_time = self._get_response(\n","                    query=example[input]\n","                )\n","\n","                if check_groundedness:\n","                    groundedness = self.check_response_groundedness(\n","                        prediction=response.get(\"response\"),\n","                        ground_truth=example[ground_truth_column],\n","                        lm=groundedness_model,\n","                    )\n","                    results.at[id, f\"{input}_groundedness\"] = groundedness[\"truthfulness\"]\n","                    results.at[id, f\"{input}_groundedness_reasoning\"] = groundedness[\"reasoning\"]\n","\n","                results.at[id, f\"{input}_response\"] = response.get(\"response\")\n","                results.at[id, f\"{input}_time\"] = response_time\n","\n","                time.sleep(0.5)\n","\n","        return results\n","\n","    def _get_response(self, query: str) -> tuple[dict, float]:\n","        \"\"\"\n","        Retrieves the chatbot's response for a given query.\n","\n","        Args:\n","            query (str): The query to send to the chatbot.\n","\n","        Returns:\n","            tuple[dict, float]: A tuple containing the response dictionary and the response time.\n","\n","        \"\"\"\n","        session_id = self.sessions_client.build_session_id(agent_id=self.agent_id)\n","        output = {}\n","\n","        response_start = time.time()\n","        res = self.sessions_client.detect_intent(agent_id=self.agent_id, session_id=session_id, text=query)\n","        turn = proto.Message.to_dict(\n","            res,\n","            use_integers_for_enums=False,\n","            including_default_value_fields=False,\n","            preserving_proto_field_name=True,\n","        )\n","        response_time = time.time() - response_start\n","\n","        output[\"response\"] = self.format_response_messages(turn)\n","\n","        # Could also grab params to check from the response\n","        # output[\"param_name\"] = turn.get(\"parameters\", {}).get(\"param_name\")\n","\n","        return output, response_time\n","\n","\n","    @staticmethod\n","    def format_response_messages(response: dict[str, Any]) -> str:\n","        \"\"\"\n","        Formats the response messages received from the chatbot.\n","\n","        Args:\n","            response (dict[str, Any]): The response dictionary.\n","\n","        Returns:\n","            str: The formatted response messages.\n","\n","        \"\"\"\n","        messages = response[\"response_messages\"]\n","\n","        if len(messages) == 0:\n","            return \"\"\n","\n","        all_messages = []\n","        for m in messages:\n","            match m:\n","                case {\"text\": {\"text\": texts}}:\n","                    all_messages.append(\"\\n\".join(texts))\n","                case {\"payload\": {\"ujet\": {\"buttons\": buttons}}}:\n","                    all_messages.append(\"\\n\".join(f\"[ {b['title']} ]\" for b in buttons))\n","                case {\"end_interaction\": info}:\n","                    all_messages.append(\"<END SESSION>\")\n","                case _:\n","                    all_messages.append(m)\n","\n","        print(all_messages)\n","        return \"\\n\".join(all_messages)\n","\n","\n","    @staticmethod\n","    def check_response_groundedness(\n","        prediction: str,\n","        ground_truth: str,\n","        lm: GenerativeModel | str = \"gemini-1.5-pro-001\",\n","        safety_settings: Optional[dict[str, Any]] = None,\n","        generation_config: Optional[dict[str, Any]] = None,\n","    ) -> dict[str, bool | str]:\n","        \"\"\"\n","        Checks the groundedness of the chatbot's response.\n","\n","        Args:\n","            prediction (str): The predicted response.\n","            ground_truth (str): The ground truth response.\n","            lm (GenerativeModel | str): The groundedness model to use. Defaults to \"gemini-1.5-pro-001\".\n","            safety_settings (Optional[dict[str, Any]]): The safety settings for the model. Defaults to None.\n","            generation_config (Optional[dict[str, Any]]): The generation config for the model. Defaults to None.\n","\n","        Returns:\n","            dict[str, bool | str]: A dictionary containing the groundedness information.\n","\n","        \"\"\"\n","        if lm is None:\n","            lm = \"gemini-1.5-pro-001\"\n","\n","        if isinstance(lm, str):\n","            lm = GenerativeModel(lm)\n","\n","        if generation_config is None:\n","            generation_config = {\n","                \"max_output_tokens\": 128,\n","                \"temperature\": 0,\n","                \"response_mime_type\": \"application/json\",\n","            }\n","\n","        prompt = f\"\"\"Given the following prediction and ground truth, determine if the prediction is true or false. Provide a clear and concise reasoning for your decision.\n","**NOTE:**\n","- The prediction might use slightly different wording than the ground truth. Compare the meaning of the prediction with that of the ground truth to determine its truthfulness.\n","\n","**Input:**\n","- **Prediction:** {prediction}\n","- **Ground Truth:** {ground_truth}\n","\n","**Output Format:**\n","{{\"truthfulness\": [True/False], \"reasoning\": [Provide a concise explanation of why the prediction is True/False based on the ground truth context.]}}\n","\n","**Example:**\n","\n","**Prediction:** The capital of France is Paris.\n","**Ground Truth:** France is a country located in Western Europe. Its capital city is Paris.\n","\n","**Example Output:**\n","{{\"truthfulness\": True, \"reasoning\": \"The ground truth explicitly states that Paris is the capital city of France, which aligns with the prediction.\"}}\n","\n","REMEMBER:\n","- ONLY OUTPUT YOUR ANSWER FORMATTED AS A DICTIONARY.\n","- DO NOT OUTPUT ANY OTHER TEXT.\n","- DO NOT INCLUDE CODE BLOCKS.\n","- THE FIRST CHARACTER OF YOUR RESPONSE SHOULD BE '{{'.\n","\"\"\"\n","        result = lm.generate_content(\n","            prompt,\n","            safety_settings=safety_settings,\n","            generation_config=generation_config,\n","        )\n","\n","        return json.loads(result.candidates[0].content.parts[0].text.strip())"]},{"cell_type":"markdown","metadata":{"id":"N2usZFhy4wSH"},"source":["# Usage Example"]},{"cell_type":"code","execution_count":227,"metadata":{"executionInfo":{"elapsed":763,"status":"ok","timestamp":1718655832166,"user":{"displayName":"Simon Mitchell-Wolf","userId":"06079501617467572736"},"user_tz":240},"id":"SBsx9ljWxuCb"},"outputs":[],"source":["# Initialize evaluator class\n","evaluator = BotEvaluator(\n","    project_id=PROJECT_ID,\n","    vertexai_region=PROJECT_REGION,\n","    agent_id=\"projects/ai-ml-team-sandbox/locations/us-east1/agents/fac86b77-f640-41f6-937b-22f037d6cc22\",\n",")\n"]},{"cell_type":"code","execution_count":228,"metadata":{"executionInfo":{"elapsed":207,"status":"ok","timestamp":1718655833044,"user":{"displayName":"Simon Mitchell-Wolf","userId":"06079501617467572736"},"user_tz":240},"id":"QpAtjQZuuc0k"},"outputs":[],"source":["# Load and clean/transform data separately with pd.read_csv()\n","dataset = pd.DataFrame([[\"What's the capital of Oklahoma?\", \"Oklahoma City\"]], columns=[\"question\", \"truth\"])\n","\n","# Or else save as test_set.csv and the class will look for that by default"]},{"cell_type":"code","execution_count":229,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3613,"status":"ok","timestamp":1718655837458,"user":{"displayName":"Simon Mitchell-Wolf","userId":"06079501617467572736"},"user_tz":240},"id":"0ntwsmHuuTtQ","outputId":"4297c342-173b-4294-8851-4f820d6b3a58"},"outputs":[{"name":"stderr","output_type":"stream","text":["\r0it [00:00, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["['Oklahoma City OK', 'END SESSION']\n"]},{"name":"stderr","output_type":"stream","text":["1it [00:03,  3.38s/it]\n"]}],"source":["# Returns a copy of the original data with merged evaluation\n","evaluation_results = evaluator.evaluate(\n","    dataset=dataset,\n","    input_columns=[\"question\"],\n","    check_groundedness=True,\n","    ground_truth_column=\"truth\",\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mxJyWjCRi1d-"},"outputs":[],"source":["# Check results\n","evaluation_results.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rKHIw2sF6B3x"},"outputs":[],"source":["from datetime import datetime\n","import pytz\n","\n","# Write results out to a timestamped file\n","\n","with open(\n","    f\"test_results_{datetime.now(pytz.utc).isoformat('T', 'seconds')}.csv\",\n","    \"w\",\n",") as f:\n","    evalutation_results.to_csv(f, index=True)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
